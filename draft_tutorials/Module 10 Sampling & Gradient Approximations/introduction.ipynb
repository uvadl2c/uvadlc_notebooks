{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f68565-79be-42c0-a858-376e0d4bf382",
   "metadata": {},
   "source": [
    "# Sampling Discrete Structures\n",
    "\n",
    "In these tutorials we discuss methods for sampling discrete variables from unstructured vectors to more structured objects such as subsets, permutations and graphs which can be incorporated in differentiable models.\n",
    "\n",
    "At the foundation of these methods are continuous relaxations for discrete (binary or categorical) random variables. \n",
    "So the first part of the tutorial gives an introduction to sampling form discrete distributions with the Gumbel-Softmax trick.\n",
    "We use the method to train a variational autoencoder with categorical latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f19ff-978e-48b6-8ba1-cc914c1e8541",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Categorical Sampling with Gumbel-Argmax\n",
    "\n",
    "Suppose we are given a categorical distribution with $C$ values as weights $w_i \\in (0,\\infty)$. \n",
    "We would like to obtain a sample from this distribution.\n",
    "The probablity of each category $c_i$ is given by the following softmax distribution\n",
    "$$p_i = \\frac{\\exp(\\log(w_i))}{\\sum_j \\exp(\\log(w_j))}$$\n",
    "\n",
    "The Gumbel-Argmax method for sampling this distribution is the following:\n",
    "Sample $U_k \\sim Uniform(0,1)$ iid and compute $r_k = \\log\\alpha_k -log(-\\log U_k)$. Then choose the index $i$ of the maximum $r_k$ (ie take the argmax) and return the 1-hot vector with the $i$th index set to 1 and the rest to 0.\n",
    "The form of noise $-log(-\\log U_k)$ added to form $r_k$ has a Gumbel distribution whence the method gets its name. The cumulative distribution function of the Gumbel distribution (with location 0 and scale 1) is given as\n",
    "$$F(z) = \\exp(\\exp(-z))$$\n",
    "\n",
    "You can take a look at a proof that this indeed samples from the softmax distribution [here](https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/).\n",
    "\n",
    "In short sampling a categorical variable with the Gumbel reparameterization proceeds as follows.\n",
    "1. Given weights $w_i$ compute $r_i = w_i+g_i$ where $g_i$ are iid Gumbel samples\n",
    "2. Argmax: Return index of largest $r_i$ as a 1-hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6908057-d655-4eae-9787-c93443320ed6",
   "metadata": {},
   "source": [
    "## Softmax Relaxation\n",
    "\n",
    "The above procedure still cannot be used in a differentiable model since the argmax operation has zero gradient except at points of discontinuity. \n",
    "So instead we use the `softmax` as a differentiable approximation to the argmax.\n",
    "In order to control the approximation we introduce a tunable temperature hyperparameter $\\tau$ which controls how far the softmax outputs are from being 1-hot. \n",
    "\n",
    "$$p_i = \\frac{\\exp(r_i/\\tau)}{\\sum_j \\exp(r_j/\\tau)}$$\n",
    "\n",
    "The following figure ([Paper Link](https://arxiv.org/abs/1611.01144)) shows the effect of temperature on the distribution and samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661a82a-dfba-4f1d-8665-10b5ae64df82",
   "metadata": {},
   "source": [
    "\n",
    "![Image](img/temp.png)\n",
    "\n",
    "\n",
    "A smaller temperature indicates a tigther approximation and the larger the temperature the looser the approximation.\n",
    "Of course, if the temperature is too small we wouldn't be able to train the model since the gradients would be very small.\n",
    "On the other hand, a large temperature would make the categorical outputs very far from being discrete, so it is important to choose an appropriate temperature for the problem at hand.\n",
    "One possiblity is to slowly anneal the temperature from large to small so that close to the end of training the relaxed categorical outputs are closed to discrete.\n",
    "In practice, however, the temperature is often kept fixed during each training trial and tuned with cross-validation.\n",
    "\n",
    "With the softmax relaxation the sampling then proceeds as follows\n",
    "\n",
    "1. Given weights $w_i$ compute $r_i = w_i+g_i$ where $g_i$ are iid Gumbel samples\n",
    "2. Apply softmax with temperature to obtain a relaxed categorical sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c60e8e-d359-4c53-9250-4b8fbb5d06f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Categorical VAE\n",
    "\n",
    "As an example of the Gumbel Softmax relaxation we show a VAE with a categorical variable latent space for MNIST. The latent space has the structure of a vector of categorical variables each with the same fixed number of categories. \n",
    "In the following example the latent space has 30 categorical variables each of dimension 10.\n",
    "Since this is a VAE we also need to define a prior on the latent space which we define to be the uniform categorical distribution.\n",
    "\n",
    "The following implementation uses code from [here](https://github.com/YongfeiYan/Gumbel_Softmax_VAE) with minor modification.\n",
    "\n",
    "We being with the required imports and hyperparameter definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb146f4-a114-4050-8e43-a628b1104547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical\n",
    "%matplotlib inline\n",
    "\n",
    "cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c02a9c-63f5-4ac1-9215-4c7ba7f6b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "latent_dim = 30\n",
    "categorical_dim = 10 \n",
    "temp = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d79a78-3b38-4012-9e0e-8bbe5de1a005",
   "metadata": {},
   "source": [
    "### Gumbel Sampling\n",
    "\n",
    "We now define the Gumbel-Softmax sampling routines. The `sample_gumbel` function samples scale 0 location 1 Gumbel variables by sampling uniform random variables in $U(0,1)$ and computing $-\\log(-\\log(U(0,1))$.\n",
    "The categorical parameters are input as unnormalized log probabilities.\n",
    "The `gumbel_softmax_sample` function adds the Gumbel noise to the logits, applies the temperature and the softmax function.\n",
    "In the `gumbel_softmax` function we also add evaluation code which simply returns a sample (unrelaxed) from the categorical distribution parameterized by `logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699a35d-5cf5-42f1-ac75-c8e998d115c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    if cuda:\n",
    "        U = U.cuda()\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, evaluate=False):\n",
    "    if evaluate:\n",
    "        d =  OneHotCategorical(logits=logits.view(-1, latent_dim, categorical_dim))\n",
    "        return d.sample().view(-1, latent_dim * categorical_dim)\n",
    "    \n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    return y.view(-1, latent_dim * categorical_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c07151-1f06-4779-ba49-12dcd015daa4",
   "metadata": {},
   "source": [
    "### VAE model\n",
    "Now we define the VAE model.\n",
    "The encoder computes the categorical probability parameters from which relaxed categorical variables can be sampled and passed into the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c5702-b438-4501-9295-ea968ae1b2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE_gumbel(nn.Module):\n",
    "    def __init__(self, temp):\n",
    "        super(VAE_gumbel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, latent_dim * categorical_dim)\n",
    "\n",
    "        self.fc4 = nn.Linear(latent_dim * categorical_dim, 256)\n",
    "        self.fc5 = nn.Linear(256, 512)\n",
    "        self.fc6 = nn.Linear(512, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        return self.relu(self.fc3(h2))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h4 = self.relu(self.fc4(z))\n",
    "        h5 = self.relu(self.fc5(h4))\n",
    "        return self.sigmoid(self.fc6(h5))\n",
    "\n",
    "    def forward(self, x, temp, evaluate=False):\n",
    "        q = self.encode(x.view(-1, 784))\n",
    "        q_y = q.view(q.size(0), latent_dim, categorical_dim)\n",
    "        z = gumbel_softmax(q_y, temp, evaluate)\n",
    "        return self.decode(z), F.softmax(q_y, dim=-1).reshape(*q.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddc330-bafe-4219-adbb-49a33aa76666",
   "metadata": {},
   "source": [
    "### KL Divergence\n",
    "Since this is a VAE we also need to compute a KL divergence of the latent probability distribution with the uniform prior $p(x) = 1/C$ for all $x$ where $C$ is the total number of categories.\n",
    "\n",
    "\\begin{align}\n",
    "KLD(q||p) &= E_q\\left[\\log\\frac{q(x)}{p(x)}\\right]\\\\\n",
    "        &= \\sum_{i=1}^{C} q(x_i) \\log (C \\cdot q(x_i))\n",
    "\\end{align}\n",
    "\n",
    "Here $q(x)$ is the latent probability distribution\n",
    "\n",
    "Finally we compute the reconstruction loss of the input as the binary cross entropy between the reconstruction parameters and the input image and add that to the KL divergence to get the VAE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea66fef-fa7d-4eeb-a265-27cc0d9a1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, qy):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False) / x.shape[0]\n",
    "\n",
    "    log_ratio = torch.log(qy * categorical_dim + 1e-20)\n",
    "    KLD = torch.sum(qy * log_ratio, dim=-1).mean()\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd23c6-42ea-4193-ac86-e1ecc20c10a0",
   "metadata": {},
   "source": [
    "Next we build the model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7961e6e-a111-493e-80ac-32da4721b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_gumbel(temp)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d889a83-9e14-4d94-a8f3-c85c601bee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a45308-ac3e-4791-8de6-7008ca10ac06",
   "metadata": {},
   "source": [
    "For training we compute both the relaxed and the unrelaxed objective.\n",
    "The unrelaxed objective is not itself used for training. \n",
    "However, since that's the objective we want to improve, it's a good idea to also evaluate it so that we can observe how far the relaxed objective is from the actual objective.\n",
    "This also allows us to get an idea of how low or high to set the temperature so that the relaxed objective is not too far from the true objective while achieving reasonable training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f66cb7-2477-48ab-b24e-bb8620364678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_loss_unrelaxed = 0\n",
    "    #temp = args.temp\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, qy = model(data, temp)\n",
    "        loss = loss_function(recon_batch, data, qy)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * len(data)\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Unrelaxed training objective for comparison\n",
    "        recon_batch_eval, qy_eval = model(data, temp, evaluate=True)\n",
    "        loss_eval = loss_function(recon_batch_eval, data, qy_eval)\n",
    "        train_loss_unrelaxed += loss_eval.item() * len(data)\n",
    "        \n",
    "    print('====> Epoch: {} Average loss relaxed: {:.4f} Unrelaxed: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset) , train_loss_unrelaxed / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d514d-e13c-4edd-bb81-ddc3cdbe013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        recon_batch, qy = model(data, temp, evaluate=True)\n",
    "        test_loss += loss_function(recon_batch, data, qy).item() * len(data)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa6ab2-1726-44cf-b8ac-ae66c5a0c91a",
   "metadata": {},
   "source": [
    "Finally we can run the training. \n",
    "You can try training with different values of the temperature to see how that affects the relaxed objective relative to the true one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9c052-5617-4304-a486-21db97d152da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3ab26-201a-40d1-bbb2-d078f457b3e1",
   "metadata": {},
   "source": [
    "### Generating Samples\n",
    "We can now generate some samples from the trained decoder.\n",
    "For this we sample some uniform categorical variables from the prior and pass them into the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3ca73-085f-4419-9be5-a3677194a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples():\n",
    "    #generate uniform probability vector\n",
    "    model.eval()\n",
    "    probs = torch.ones([64, latent_dim, categorical_dim])*(1/categorical_dim)\n",
    "    cat_samples = OneHotCategorical(probs=probs.cuda()).sample().view(-1, latent_dim*categorical_dim)\n",
    "    output = model.decode(cat_samples)\n",
    "    return output.view(-1,28,28).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74eaa7d-cac5-4c90-b13d-f6036d9a6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generate_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a465d-34c3-46ed-824b-a206adf55dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_gray_image_grid\n",
    "%matplotlib inline\n",
    "show_gray_image_grid(samples, 8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafaa226-a9a7-481c-b58f-0631a543b533",
   "metadata": {},
   "source": [
    "## Gumbel Straight-Through\n",
    "\n",
    "In some instances we want to train models with discrete variables but do not want to pass relaxed values as inputs.\n",
    "This might be the case where we want to optimize a function that cannot be defined for relaxed inputs and must use categorical/discrete inputs.\n",
    "One heuristic for such cases is the Straight-Through estimator.\n",
    "Here given some pre-activation $y$ for which we want the gradient we compute the sample $z$ using the non-differentiable sampling operations such as with categorical or Bernoulli sampling.\n",
    "Then we compute the downstream function $f$ on the hard sample.\n",
    "Then in the backward pass we ignore the non-differentiable operation and pass the gradient relative to $z$ back as the gradient relative to $y$. That is we set.\n",
    "$$\\partial_y f := \\partial_z f.$$\n",
    "This is the straight-through gradient and is biased, but in many cases this can often allow us to train models with discrete variables.\n",
    "\n",
    "Here we use the straight-through gradient with Gumbel-Softmax relaxation. \n",
    "In this case we compute the Gumbel-Softmax relaxation as before.\n",
    "From the relaxation we compute a 1-hot vector where the index with the largest value is set to 1 and all others are set to 0.\n",
    "This is used as the discrete value in the downstream network and we use the straight-through gradient in the backward pass.\n",
    "\n",
    "Given a hard vector `y_hard` and a soft vector `y` there is a well-known trick that can be used to incorporate straight-through gradients.\n",
    "We compute\n",
    "```\n",
    "y = (y_hard - y).detach() + y\n",
    "```\n",
    "which simply uses the `y_hard` in the forward pass but the gradient relative to `y` in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3befe-a6a3-473d-b69b-ce1e2095fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gumbel_softmax(logits, temperature, evaluate=False, hard=False):\n",
    "    if evaluate:\n",
    "        d =  OneHotCategorical(logits=logits.view(-1, latent_dim, categorical_dim))\n",
    "        return d.sample().view(-1, latent_dim * categorical_dim)\n",
    "    \n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if hard:\n",
    "        #Straight-through gradient\n",
    "        #takes the index of the largest and insert a 1.\n",
    "        #all others are set to 0 obtaining a 1-hot vector.\n",
    "        shape = logits.size()\n",
    "        _, k = y.max(-1)\n",
    "        \n",
    "        y_hard = torch.zeros_like(logits)\n",
    "        y_hard = y_hard.zero_().scatter_(-1, k.view(shape[:-1] + (1,)), 1.0)\n",
    "        \n",
    "        #This a trick to use the 1-hot value in the forward pass and the \n",
    "        #relaxed gradient in the backward pass\n",
    "        y = (y_hard - y).detach() + y\n",
    "    \n",
    "    return y.view(-1, latent_dim * categorical_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e85ac6-4186-49c5-a980-6ee1ea589212",
   "metadata": {},
   "source": [
    "As an exercise you can train the VAE model given above using Gumbel-Straight-Through by using the above function in the model definition and setting `hard=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439bd6d4-d7c0-4556-a4a1-3762523d1cfe",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Categorical Reparameterization with Gumbel-Softmax](https://arxiv.org/abs/1611.01144)\n",
    "\n",
    "[The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables](https://arxiv.org/abs/1611.00712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1d7e3-bd59-466c-962b-322ca9d7e949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
