{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba3ec69d-904c-42d2-8976-f1ff789b72c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sampling Subsets with Gumbel-Top $k$ Relaxations\n",
    "\n",
    "In this part we show how to include a subset sampling component in differentiable models using Gumbel Top $k$ relaxations.\n",
    "First we show how to build a differentiable subset sampler and then we show one application to differentiable $k$ nearnest neighbor classification.\n",
    "\n",
    "Formally speaking we are given $N$ elements with weights $w_i$.\n",
    "We would like to sample $k$ elements from $N$ without replacement.\n",
    "Stated otherwise, we want a $k$-element subset $S=\\{w_{i_1}, w_{i_2},\\ldots, w_{i_k}\\}$ from $N$ elements.\n",
    "\n",
    "Given total weight $Z=\\sum w_i$, the first element is sampled with probability $\\frac{w_{i_1}}{Z}$, the second with probability $\\frac{w_{i_2}}{Z-w_{i_1}}$ and so on for $k$ elements.\n",
    "Multiplying the factors gives the following distribution for $k$ element subsets.\n",
    "\n",
    "$$ p(S) = \\frac{w_{i_1}}{Z}  \\frac{w_{i_2}}{Z-w_{i_1}}\\cdots \\frac{w_{i_k}}{Z-\\sum_{j=1}^{k-1} w_{i_j}}.$$\n",
    "\n",
    "In the introduction we showed how sampling from a categorical distribution could be recast as choosing the argmax of a set of Gumbel random variables.\n",
    "Relaxing the argmax with a softmax allowed us to approximate sampling from the target categorical distribution. \n",
    "A temperature could be used to control the extent of relaxation.\n",
    "In this case the the categorical probabilities are given by the softmax distrbution\n",
    "$$p_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)} = \\frac{w_i}{\\sum_j w_j}$$\n",
    "\n",
    "It turns out that by selecting the $k$ largest Gumbel random variables instead of just the largest we can sample subsets according to the sampling without replacement probability given above.\n",
    "This procedure is closely related to a procedure known by the name of weighted reservoir sampling.\n",
    "\n",
    "Seen this way, the Gumbel-Argmax trick is a method for sampling subsets of size $k=1$ with probabilities given by $p_i$.\n",
    "Replacing the argmax by a Top-$k$ procedure for selecting the $k$ largest elements generalizes the Gumbel-Argmax to sample size $k$ subsets with probablity $p(S)$.\n",
    "In this case we think of the Top-$k$ procedure as returning a $k$-hot vector $y$ where $y_i=1$ if the $i$th element is selected and $y_i=0$ otherwise.\n",
    "Thus we represent subsets as $k$-hot vectors which also generalizes the representation of categorical samples as 1-hot vectors.\n",
    "\n",
    "The unrelaxed subset sampling procedure can then be written as follows given non-negative weights $w_i$.\n",
    "\n",
    "1. Compute keys $\\hat{r_i} = -\\log(-\\log(u_i)) +  \\log(w_i)$ for all $i$ and $u_i \\in U(0,1)$.\n",
    "2. Return $k$ largest keys $\\hat{r_i}$.\n",
    "\n",
    "## Top $k$ Relaxation\n",
    "\n",
    "We can construct an unrelaxed Top $k$ by iteratively applying the softmax $k$ times and sampling a 1-hot categorical sample at each step.\n",
    "The $k$ 1-hot categorical samples are then combined into a single $k$-vector.\n",
    "When the categorical sample gives a particular element, the log probablity for that element is set to $-\\infty$ for the future iterations so that the element is never chosen again. We can relax this procedure by replacing samples from the softmax by the probabilties computed by softmax. When the softmax temperature is set to be small, the sampled and the relaxed outputs are close.\n",
    "\n",
    "In more detail the procedure is as follows.\n",
    "\n",
    "### Unrelaxed Version\n",
    "For $i=1\\ldots n$ and $j=1\\ldots k$, set $ \\alpha^1_i = \\hat{r_i}$ and $\\alpha_i^{j+1} = \\alpha_i^{j} + \\log(1-a_i^j)$\n",
    "\n",
    "Here $a^j_i$ is a sample the categorical distribution with probabilities $p(a^j_i = 1) = \\frac{\\exp(\\alpha_i^{j}/\\tau)}{\\sum_k\\exp(\\alpha_k^{j}/\\tau)}$ and $\\tau$ is a temperature.\n",
    "\n",
    "Note that when $a_i^j$ is a 1-hot categorical sample the $\\log(1-a_i^j)$ term in the first equation above sets the next $\\alpha_i^{j+1}$ to $-\\infty$ if $a_i^j=1$ and leaves it unchanged otherwise.\n",
    "This ensures that the $i$th element once sampled is not sampled in the next steps.\n",
    "Finally we add all the $k$ vectors as $\\sum_j a^j$ and return the output as the sample.\n",
    "\n",
    "\n",
    "### Relaxed Version\n",
    "To relax the above procedure we can replace the categorical sample at step by its expectation.\n",
    "In this case the update becomes\n",
    "\n",
    "For $i=1\\ldots n$ and $j=1\\ldots k$, set $ \\alpha^1_i = \\hat{r_i}$ and $\\alpha_i^{j+1} = \\alpha_i^{j} + \\log(1-p(a_i^j=1))$\n",
    "\n",
    "where $p(a^j_i = 1) = \\frac{\\exp(\\alpha_i^{j}/\\tau)}{\\sum_k\\exp(\\alpha_k^{j}/\\tau)}$ as above.\n",
    "At low values of $\\tau$ the softmax distribution becomes close to deterministic outputs a value that is close to $k$-hot.\n",
    "The temperature variable is a hyperparameter and ideally should be annealed from larger to smaller values during the course of training.\n",
    "However, in most applications the temperature is left fixed per trial and tuned using cross validation.\n",
    "Proper tuning of temperature can have a significant effect on the performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56efb3d9-e5c6-4e76-8ba2-a46ee9ab799a",
   "metadata": {},
   "source": [
    "In the following we use code from [[here](https://github.com/ermongroup/subsets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff79081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import one_hot\n",
    "from dataset import DataSplit\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de11c02-838a-4fd1-98e6-33f141b63ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1fb29-e6f5-4d3c-9b57-e1ca51998b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Subset Sampler Class\n",
    "\n",
    "The following `SubsetOperator` class implements the relaxed subset sampling procedure described above.\n",
    "As described the `forward` method takes a list of scores (unormalized log probs) of some fixed dimension.\n",
    "We add Gumbel noise with location 0 and scale 1 and divide by the temperature.\n",
    "Next we apply the Top-$k$ relaxation and return the resulting $k$-hot vector as the sampled subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673e9e7-567c-4555-9af0-3d6bbbf6bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = np.finfo(np.float32).tiny\n",
    "\n",
    "class SubsetOperator(torch.nn.Module):\n",
    "    def __init__(self, k, tau=1.0, hard=False):\n",
    "        super(SubsetOperator, self).__init__()\n",
    "        self.k = k\n",
    "        self.hard = hard\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, scores):\n",
    "        m = torch.distributions.gumbel.Gumbel(torch.zeros_like(scores), torch.ones_like(scores))\n",
    "        g = m.sample()\n",
    "        scores = scores + g\n",
    "\n",
    "        # continuous top k\n",
    "        khot = torch.zeros_like(scores)\n",
    "        onehot_approx = torch.zeros_like(scores)\n",
    "        for i in range(self.k):\n",
    "            khot_mask = torch.max(1.0 - onehot_approx, torch.tensor([EPSILON]).cuda())\n",
    "            scores = scores + torch.log(khot_mask)\n",
    "            onehot_approx = torch.nn.functional.softmax(scores / self.tau, dim=1)\n",
    "            khot = khot + onehot_approx\n",
    "\n",
    "        if self.hard:\n",
    "            # will do straight through estimation if training\n",
    "            khot_hard = torch.zeros_like(khot)\n",
    "            val, ind = torch.topk(khot, self.k, dim=1)\n",
    "            khot_hard = khot_hard.scatter_(1, ind, 1)\n",
    "            res = khot_hard - khot.detach() + khot\n",
    "        else:\n",
    "            res = khot\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d1cef-ac14-4ad4-b812-a3d30b04d90b",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can try the sampler on some example input and various temperatures. Note that the sum of the vectors elements is always $k$.\n",
    "At lower temperatures the output should be close to $k$-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e44af5-2059-46b2-8306-076d27fa6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SubsetOperator(k=2, tau=1.0)\n",
    "\n",
    "x = torch.tensor([[1.,2.,3.,4.]]).to(gpu)\n",
    "y = sampler(x)\n",
    "print(y, y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d96735-01c3-415b-b950-ba79fdcf98b3",
   "metadata": {},
   "source": [
    "## Empirical Sampling Distribution\n",
    "\n",
    "We empirically confirm that the $k$-hot relaxation generates subsets with the same distribution as the sampling without replacement distribution.\n",
    "For this we define a set with weights in `[1,2,3,4]` and generate 10000 subsets of size 2 using the true distribution (here with Gumbel Top $k$).\n",
    "Then we generate subsets using the relaxation given above with a fixed temperature.\n",
    "The samples are plotted side-by-side in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbb29d-2b50-4d9d-8ab8-26f01b16f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([[1.,2.,3.,4.]]).to(gpu)\n",
    "w = w.tile((10000,1))\n",
    "w_scores = torch.log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d202fc-1614-46c3-a245-b7ade356c5e3",
   "metadata": {},
   "source": [
    "Use Gumbel-Top-$k$ to get true distribution samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce98b8-c13d-4af7-9b8e-d808b44d8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#true samples\n",
    "m = torch.distributions.gumbel.Gumbel(torch.zeros_like(w_scores), torch.ones_like(w_scores))\n",
    "g = m.sample()\n",
    "scores = w_scores + g\n",
    "samples = torch.topk(scores, 2)[1]\n",
    "samples = samples.detach().cpu().numpy()\n",
    "samples = [str(x) for x in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c853b6-0f5d-46ab-bfe1-ea99e83f51a1",
   "metadata": {},
   "source": [
    "To get samples of subsets from the relaxation, we first apply the relaxation and choose the Top $k$ indices as the chosen subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6f194-9bee-47a7-b589-4cf3cf92c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relaxed samples\n",
    "r_samples = sampler(w_scores)\n",
    "\n",
    "r_samples = torch.topk(r_samples, 2)[1]\n",
    "r_samples = r_samples.detach().cpu().numpy()\n",
    "r_samples = [str(x) for x in r_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2222b33-ae56-4750-91c7-feca127b7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([samples,r_samples], align='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296b43-0a91-4b67-88d6-ba087f6ac35f",
   "metadata": {},
   "source": [
    "## Application: Differentiable k Nearest Neighbors\n",
    "\n",
    "Now we apply the subset sampling procedure to a classification problem with differentiable $k$ nearest neighbors. \n",
    "Recall that in $k$ nearest neighbors classification we look at the labels of the $k$ nearest neighbors and take the majority vote for classication.\n",
    "Unlike the classical form of nearest neigbors, we want to take the feature from a deep network.\n",
    "The $k$ nearest neighbors loss is cannot be directly used in differentiable models so we relax it with our subset relaxation.\n",
    "Furthermore instead of looking for the nearest neighbors in the entire dataset (which can be large) we choose a random subset of data points for the distance calculations.\n",
    "\n",
    "Given a query vectory $q$ and a set $N$ of neighbors we compute the Euclidean distance between the $q$ and each element $i \\in N$.\n",
    "This gives us a list of scores (negative of the distances) and sample a $k$ size subset of these scores as a relaxed $k$-hot vector.\n",
    "\n",
    "Since this is a classification problem, during training we have the label $y$ for the query vector $q$ and for each of the neighbors $y_i$.\n",
    "If the labels are equal for a query, neighbor pair, we include the corresponding score otherwise we set it to 0 and take the sum to compute the loss.\n",
    "\n",
    "Given a subset of $k$ neighbors the loss can be written as \n",
    "$$L(S;q) = -\\sum_{j\\in S} I[y==y_i].$$ \n",
    "\n",
    "The actual loss is then the expectation of this expression over all subsets.\n",
    "$$ L(q) = E_S[L(S;q)].$$\n",
    "\n",
    "The `SubsetsDKNN` class computes the scores between the queries and neighbors and returns the sampled subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ddbcc4-5b57-412b-9514-5c4480b35bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetsDKNN(torch.nn.Module):\n",
    "    def __init__(self, k, tau=1.0, hard=False, num_samples=-1):\n",
    "        super(SubsetsDKNN, self).__init__()\n",
    "        self.k = k\n",
    "        self.subset_sample = SubsetOperator(k=k, tau=tau, hard=hard)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    # query: batch_size x p\n",
    "    # neighbors: 10k x p\n",
    "    def forward(self, query, neighbors, tau=1.0):\n",
    "        diffs = (query.unsqueeze(1) - neighbors.unsqueeze(0))\n",
    "        squared_diffs = diffs ** 2\n",
    "        l2_norms = squared_diffs.sum(2)\n",
    "        norms = l2_norms  # .sqrt() # M x 10k\n",
    "        scores = -norms\n",
    "\n",
    "        top_k = self.subset_sample(scores)\n",
    "        return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de000c-51d0-44b9-a3c3-75c9b883f9b4",
   "metadata": {},
   "source": [
    "The following is the convNet that we use to compute the features of the data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101e91a-4aea-45d5-afe2-cc14a80ae057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, stride=1)\n",
    "        self.linear = nn.Linear(800, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.linear(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68244399-c24f-4e05-b5e9-85c1bfb09be7",
   "metadata": {},
   "source": [
    "Define hyperparameters.\n",
    "Here we say that we are going to using 100 queries per minibatch and 100 neighbors for the distance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5940d1b-755c-48f7-9ded-8dae54e6e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9\n",
    "tau = 1.0\n",
    "NUM_TRAIN_QUERIES = 100\n",
    "NUM_TEST_QUERIES = 10\n",
    "NUM_TRAIN_NEIGHBORS = 100\n",
    "LEARNING_RATE = 10 **-3\n",
    "NUM_EPOCHS = 20\n",
    "EMBEDDING_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831aa000-ee01-43fa-889a-996926efa2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dknn_layer = SubsetsDKNN(k, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb691a-631f-407c-b83e-93758883ad57",
   "metadata": {},
   "source": [
    "### Loss\n",
    "Now we compute the loss given the sampled subsets for the queries using the labels for the queries and neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22c34c-afb7-43a9-a947-32bc23a14b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dknn_loss(query, neighbors, query_label, neighbor_labels):\n",
    "    # query: batch_size x p\n",
    "    # neighbors: 10k x p\n",
    "    # query_labels: batch_size x [10] one-hot\n",
    "    # neighbor_labels: n x [10] one-hot\n",
    "\n",
    "    # num_samples x batch_size x n\n",
    "    start = time.time()\n",
    "    top_k_ness = dknn_layer(query, neighbors)\n",
    "    elapsed = time.time() - start\n",
    "    correct = (query_label.unsqueeze(1) *\n",
    "               neighbor_labels.unsqueeze(0)).sum(-1)  # batch_size x n\n",
    "    correct_in_top_k = (correct.unsqueeze(0) * top_k_ness).sum(-1)\n",
    "    loss = -correct_in_top_k\n",
    "\n",
    "    return loss, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fbd25-f65f-42b7-8741-d084c31887dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_phi = ConvNet().to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82393e96-77aa-49aa-86f8-cc8f533a9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    h_phi.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf54c35-af02-4cb9-b830-66bb4094a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = DataSplit('mnist')\n",
    "\n",
    "batched_query_train = split.get_train_loader(NUM_TRAIN_QUERIES)\n",
    "batched_neighbor_train = split.get_train_loader(NUM_TRAIN_NEIGHBORS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687dca8-f649-48d8-9f74-4a0080b1f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    timings = []\n",
    "    h_phi.train()\n",
    "    to_average = []\n",
    "    # train\n",
    "    for query, candidates in zip(batched_query_train, batched_neighbor_train):\n",
    "        optimizer.zero_grad()\n",
    "        cand_x, cand_y = candidates\n",
    "        query_x, query_y = query\n",
    "\n",
    "        cand_x = cand_x.to(device=gpu)\n",
    "        cand_y = cand_y.to(device=gpu)\n",
    "        query_x = query_x.to(device=gpu)\n",
    "        query_y = query_y.to(device=gpu)\n",
    "\n",
    "        neighbor_e = h_phi(cand_x).reshape(NUM_TRAIN_NEIGHBORS, EMBEDDING_SIZE)\n",
    "        query_e = h_phi(query_x).reshape(NUM_TRAIN_QUERIES, EMBEDDING_SIZE)\n",
    "\n",
    "        neighbor_y_oh = one_hot(cand_y).reshape(NUM_TRAIN_NEIGHBORS, 10)\n",
    "        query_y_oh = one_hot(query_y).reshape(NUM_TRAIN_QUERIES, 10)\n",
    "\n",
    "        losses, timing = dknn_loss(query_e, neighbor_e, query_y_oh, neighbor_y_oh)\n",
    "        timings.append(timing)\n",
    "        loss = losses.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        to_average.append((-loss).item() / k)\n",
    "\n",
    "    print('Avg. train correctness of top k:',\n",
    "          sum(to_average) / len(to_average))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35655d4f-0c65-4eff-a296-bf72a0cdce0b",
   "metadata": {},
   "source": [
    "For testing we can directly take the $k$ nearest neighbors and do not sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61449072-d4d9-4d93-b7cd-7e76a1f64604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "\n",
    "def new_predict(query, neighbors, neighbor_labels):\n",
    "    '''\n",
    "    query: p\n",
    "    neighbors: n x p\n",
    "    neighbor_labels: n (int)\n",
    "    '''\n",
    "    diffs = (query.unsqueeze(1) - neighbors.unsqueeze(0))  # M x n x p\n",
    "    squared_diffs = diffs ** 2\n",
    "    norms = squared_diffs.sum(-1)  # M x n\n",
    "    indices = torch.argsort(norms, dim=-1)\n",
    "    labels = neighbor_labels.take(indices[:, :k])  # M x k\n",
    "    prediction = [majority(l.tolist()) for l in labels]\n",
    "    return torch.Tensor(prediction).to(device=gpu).long()\n",
    "\n",
    "\n",
    "def acc(query, neighbors, query_label, neighbor_labels):\n",
    "    prediction = new_predict(query, neighbors, neighbor_labels)\n",
    "    return (prediction == query_label).float().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8715d-9a28-4860-8928-15a1c73a4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_query_val = split.get_valid_loader(NUM_TEST_QUERIES)\n",
    "batched_query_test = split.get_test_loader(NUM_TEST_QUERIES)\n",
    "\n",
    "def test(epoch, val=False):\n",
    "    h_phi.eval()\n",
    "    global best_acc\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        for neighbor_x, neighbor_y in batched_neighbor_train:\n",
    "            neighbor_x = neighbor_x.to(device=gpu)\n",
    "            neighbor_y = neighbor_y.to(device=gpu)\n",
    "            embeddings.append(h_phi(neighbor_x))\n",
    "            labels.append(neighbor_y)\n",
    "        neighbors_e = torch.stack(embeddings).reshape(-1, EMBEDDING_SIZE)\n",
    "        labels = torch.stack(labels).reshape(-1)\n",
    "\n",
    "        results = []\n",
    "        for queries in batched_query_val if val else batched_query_test:\n",
    "            query_x, query_y = queries\n",
    "            query_x = query_x.to(device=gpu)\n",
    "            query_y = query_y.to(device=gpu)\n",
    "            query_e = h_phi(query_x)  # batch_size x embedding_size\n",
    "            results.append(acc(query_e, neighbors_e, query_y, labels))\n",
    "        total_acc = np.mean(np.array(results))\n",
    "\n",
    "    split = 'val' if val else 'test'\n",
    "    print('Avg. %s acc:' % split, total_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32dc2f-abcb-4c48-a118-fcf02972b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, NUM_EPOCHS):\n",
    "    print('Beginning epoch %d: ' % t)\n",
    "    train(t)\n",
    "test(-1, val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de83b1-1c03-4f1e-8699-da12b8fa163a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Reparameterizable Subset Sampling via Continuous Relaxations](https://arxiv.org/abs/1901.10517). [[Code](https://github.com/ermongroup/subsets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d49af2-51e7-4838-a707-743ed91d0679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
